{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selenium had to be used to collect the data from the rating because the website is dynamically rendered: you \n",
    "#have to manually select the rating you want from a menu and then you get that specific table rendered\n",
    "#with no redirection. \n",
    "\n",
    "#To get the personal data from every athlete, you must access a specific url for each one of them,\n",
    "#so we don't need to use Selenium web driver anymore and Scrapy will be used instead because of its efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv('rating.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = df_base['URL detalhes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spider(scrapy.Spider):\n",
    "    name = \"scrap\"\n",
    "    start_urls = urls\n",
    "    count = 1\n",
    "    total = len(start_urls)\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_ENABLED' : 'False',\n",
    "        'FEED_FORMAT':'csv',\n",
    "        'FEED_URI': 'rating_detalhado.csv',\n",
    "        \n",
    "        'USER_AGENT' : 'Bárbara Gomes (barbaragomes@ufmg.br)',\n",
    "        'ROBOTSTXT_OBEY' : 'True',\n",
    "        \n",
    "        'CONCURRENT_REQUESTS_PER_DOMAIN' : '32', \n",
    "        'CONCURRENT_REQUESTS' : '32',\n",
    "        'DOWNLOAD_DELAY' : '0.5',\n",
    "        'RANDOMIZE_DOWNLOAD_DELAY' : 'True',\n",
    "        \n",
    "        'AUTOTHROTTLE_ENABLED' : 'False',\n",
    "        'HTTPCACHE_ENABLED' : 'True',\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        eTime = time.time() - start_time\n",
    "        print(f'\\rParsing {self.count}/{self.total} \\\n",
    "              Elapsed minutes = {eTime/60} \\\n",
    "              Minutes left = {eTime * (self.total - self.count) / (self.count * 60)}', end='')\n",
    "        self.count += 1\n",
    "        \n",
    "        name = response.css('#lblNome::text').get()\n",
    "        state = response.css('#imgBandeiraUF ::attr(src)').get()[-6:-4] \n",
    "        club = response.css('#lblClube::text').get() \n",
    "        age = response.css('#lblIdade::text').get()\n",
    "        category = response.css('#lblCategoria::text').get()[-1]\n",
    "        rating = response.css('#lblCategoria::text').get()[1]\n",
    "        pts = response.css('#lblPontos::text').get()\n",
    "        placing = response.css('#lblColocacao::text').get()\n",
    "        \n",
    "        championships = []\n",
    "        \n",
    "        for ch in response.css('#grideventos_DXMainTable td'):\n",
    "            chData = ch.css('span::text').getall() \n",
    "            #'Ganho' (chData[3]) is missing in 4 records, so I add it to chData as NaN\n",
    "            if (len(chData) == 4):\n",
    "                chData.append(chData[3])\n",
    "                chData[3] = 'NaN'\n",
    "                \n",
    "            if chData:\n",
    "                championships.append({                \n",
    "                    'Data' : chData[0],\n",
    "                    'Campeonato' : chData[1],\n",
    "                    'Inicial' : chData[2],\n",
    "                    'Ganho' : chData[3],\n",
    "                    'Final' : chData[4]\n",
    "                })\n",
    "            \n",
    "        yield {\n",
    "            'Nome' : name,\n",
    "            'Estado' : state,\n",
    "            'Clube' : club,\n",
    "            'Idade' : age,\n",
    "            'Categoria' : category,\n",
    "            'Rating' : rating,\n",
    "            'Pontos' : pts,\n",
    "            'Colocação' : placing,\n",
    "            'URL detalhes' : response.request.url,\n",
    "            'Eventos' : championships \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(get_project_settings())\n",
    "#process = CrawlerProcess({'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'})\n",
    "process.crawl(Spider)\n",
    "start_time = time.time()\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rating_detalhado.csv')\n",
    "errorCount = len(pd.concat([df_base['URL detalhes'], df['URL detalhes']]).drop_duplicates(keep=False))\n",
    "print(f'{str(errorCount)} records couldn`t be scrapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df_base['URL detalhes'][~df_base['URL detalhes'].isin(df['URL detalhes'])].dropna().values\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
